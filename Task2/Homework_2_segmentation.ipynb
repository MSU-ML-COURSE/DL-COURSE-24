{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRaMThjN_R_T"
   },
   "source": [
    "# Задание 2: Трехклассовая семантическая сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECLnfs9p_R_V"
   },
   "source": [
    "Предлагается решить задачу семантической сегментации животных с тремя классами: класс \"фон\" (метка 0), класс \"кошка\" (метка 1) и класс \"собака\" (метка 2).\n",
    "![Image](https://miro.medium.com/max/1130/1*DDEkOFC93pEbrTdyhdpXZg.png)\n",
    "\n",
    "Для этого сами подготовим [датасет](https://drive.google.com/uc?export=download&id=1ZsRAXiPgOU5Am8tNZ7mruwtJh3ck8TI5), реализуем метрики/функции потерь, реализуем и обучим свою [PSPNet](https://arxiv.org/abs/1612.01105)-подобную архитектуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrgP6eovJTuh",
    "outputId": "b37d1f84-b3e6-4004-91c0-8b48f0fe84f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# скачиваем данные\n",
    "!gdown -q --id 1ZsRAXiPgOU5Am8tNZ7mruwtJh3ck8TI5 -O data.zip\n",
    "!unzip -qq data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoREYGgz_R_V"
   },
   "source": [
    "### Загрузка модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SXJCvBq_R_W"
   },
   "outputs": [],
   "source": [
    "# Загружаем pytorch для работы с нейронными сетями\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Для работы с изображениями/графиками\n",
    "from torchvision import transforms\n",
    "# Загружаем способы интерполяции изображений\n",
    "from torchvision.transforms.functional import InterpolationMode as IM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Для логирования метрик и функций потерь в ходе обучения\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Для удобной работы с обучающей/тестовой выборкой\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Прочее\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPPykYYs_R_X"
   },
   "source": [
    "## Часть 1: Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYunIhPF_R_d"
   },
   "source": [
    "### 1.1 Предобработка датасета (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLReC1XI_R_d"
   },
   "source": [
    "Для начала работы с данными требуется выполнить следующие пункты:\n",
    "- **Определиться со способом хранения/чтения данных с диска. Предлагается сравнить три варианта: `HDF5`, `memory-mapped files` и `\"сырой\"` вид ( хранение `.jpg/.png` файлов на диске). Все необходимые классы уже описаны в файле `utils.py`. Предлагается лишь замерить скорость чтения данных для каждого из форматов, затем выбрать наиболее быстрый.**\n",
    "    - Поговорим поподробнее об особенностях этих форматов хранения. В задачах комьютерного зрения датасеты, как правило, имеют большой размер, который не помещается в оперативную память.  Формат `hdf5` позволяет разбивать массивы информации на [chunks](https://www.oreilly.com/library/view/python-and-hdf5/9781491944981/ch04.html), которые организованы в виде [B-деревьев](https://en.wikipedia.org/wiki/B-tree). Такой вид хранения необходим для эффективного чтения `hyperslabs` - многомерных срезов массива, которые несмежны в памяти (non-contiguous). По умолчанию, `hdf5` хранит данные непрерывно (contiguous)\n",
    "    - `Memory-mapping` файлов в оперативную память позволяет пропустить этап буфферизации, тем самым пропуская операцию копирования, лениво загружая информацию напрямую. Особенность этого подхода в том, что алгоритмически `Best case` скорости чтения достигается на непрерывном блоке информации (contiguous), а `Worst case` - наоборот, на несмежном в памяти (non-contiguous) блоке (на порядки хуже, чем потенциально возможно в `hdf5`).\n",
    "\n",
    "- **Привести все пары (изображение, маска) к единому размеру `target_shape`, указанному далее в словаре конфигурации `default_config`**. Предлагается следующая последовательность действий:\n",
    "    1. **При помощи [transforms.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize) интерполировать (по умолчанию билинейная интерполяция) значения пикселей при изменении размера исходного изображения до заданного**. Однако, подобная операция искажает исходное соотношение сторон изображения, что может негативно сказаться на предсказательной способности сети. Например, общий вид морды кошки будет зависеть от исходного размера изображения, а не от сущности класса \"кошка\": оно может быть не растянуто, может быть растянуто вертикально/горизонтально. Неконсистентность в представлении одной и той же сущности может привести к нестабильному обучению, так как размеры ядра свертки едины для любого входного изображения! К счастью, эта проблема уже решена в `transforms.Resize`: при целочисленном аргументе `size` наименьшая сторона входного изображения будет интерполирована до `size`, а другая сторона (наибольшая) до размера `size * aspect_ratio`, т.е сохраняя соотношение сторон `aspect_ratio`\n",
    "    2. На текущий момент лишь одна из сторон исходного изображения соответствует требуемому размеру `target_shape`. Возможен случай когда оставшаяся сторона больше требуемого размера. Тогда необходимо **обрезать изображение при помощи [transforms.CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html#torchvision.transforms.CenterCrop)**.\n",
    "\n",
    "> Последовательное исполнение операций модуля `transforms` можно выполнить при помощи [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html).\n",
    "- **Ответить на вопрос:** `А зачем, вообще, требуется сводить все изображения к одному размеру?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_JzU6sK_R_e"
   },
   "source": [
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZPJ9WeU_R_f"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def resize(img: Image_t, target_shape: tuple[int, int]) -> np.array:\n",
    "    \"\"\"\n",
    "    Приводит входное изображение (или маску) `img` к размеру `target_shape`, указанной выше\n",
    "    последовательностью действий. Предполагается, что требуемый размер `target_shape` \"квадратный\"\n",
    "    \"\"\"\n",
    "    # Проверяем равенство желаемых ширины и высоты в target_shape\n",
    "    assert target_shape[0] == target_shape[1]\n",
    "\n",
    "    # Масштабируем наименьшую размерность `img` под `target_shape`\n",
    "    # В качестве способа интерполяции выберем интерполяцию методом ближайшего соседа\n",
    "    # Это необходимо для сохранения множества значений маски сегментации\n",
    "    img = <YOUR_CODE>  # используйте transforms.Resize\n",
    "    resize_transform = transforms.Compose([\n",
    "        # Обрезаем \"лишние\" пиксели. Если их нет, то CenterCrop ничего не изменит (случай \"меньше\").\n",
    "        <YOUR_CODE>,  # используйте transforms.CenterCrop\n",
    "\n",
    "        # Преобразуем PIL.Image изображение в массив np.array\n",
    "        <YOUR_CODE>  # используйте transforms.Lambda\n",
    "    ])\n",
    "\n",
    "    return resize_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyd3WZ38_R_f"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(config: dict, storage_class: Type[storage_class]):\n",
    "    \"\"\"\n",
    "    Предобрабатывает датасет и эффективно его сохраняет на диск\n",
    "    \"\"\"\n",
    "    with open(config[\"annotation_file\"]) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Заводим массивы для блоков изображений, помещаемых в память\n",
    "    input_chunk = np.empty((config[\"chunk_size\"], *config[\"target_shape\"], 3), dtype=np.uint8)\n",
    "    target_chunk = np.empty((config[\"chunk_size\"], *config[\"target_shape\"]), dtype=np.uint8)\n",
    "\n",
    "    # Делим датасет на блоки\n",
    "    config[\"dataset_size\"] = len(lines)\n",
    "    num_chunks = config[\"dataset_size\"] // config[\"chunk_size\"] + bool(config[\"dataset_size\"] % config[\"chunk_size\"])\n",
    "    dataset = storage_class(config)\n",
    "\n",
    "    # Читаем изображения с диска, предобрабатываем и сохраняем в выбранный нами формат\n",
    "    for chunk_idx in tqdm(range(num_chunks)):\n",
    "        for pos in range(config[\"chunk_size\"]):\n",
    "            flat_idx = chunk_idx * config[\"chunk_size\"] + pos\n",
    "            if (flat_idx >= config[\"dataset_size\"]):\n",
    "                break\n",
    "\n",
    "            img_name, label = lines[flat_idx].rstrip(\"\\n\").split(' ')\n",
    "\n",
    "            input_raw = Image.open(os.path.join(config[\"input_dir\"], img_name + \".jpg\")).convert(\"RGB\")\n",
    "            target_raw = Image.open(os.path.join(config[\"target_dir\"], img_name + \".png\")).convert('L')\n",
    "\n",
    "            input_chunk[pos] = resize(input_raw, config[\"target_shape\"])\n",
    "            target_chunk[pos] = renumerate_target(resize(target_raw, config[\"target_shape\"]), int(label))\n",
    "        dataset.append(input_chunk, target_chunk)\n",
    "    dataset.lock()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqpIqhgO_R_g"
   },
   "source": [
    "\n",
    "Для простоты будем выбирать размер изображений `target_shape` с одинаковыми сторонами. Предлагается использовать размер `256x256`, хотя выбор за вами. Обратите внимание, что от размера изображений зависит быстродействие дальнейшего кода (чем больше картинки, тем дольше обучать)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qE7809Y_R_h"
   },
   "outputs": [],
   "source": [
    "# если хотите перезапустить ячейку, возможно вам понадобится удалить некоторые файлы, раскоменьте строчку ниже\n",
    "# !rm -rf SegTask/trainval.h5 SegTask/trainval\n",
    "\n",
    "# Конфигурация датасета\n",
    "default_config = {\n",
    "             \"input_dir\": \"SegTask/images\",\n",
    "             \"target_dir\": \"SegTask/seg_masks\",\n",
    "             \"target_shape\": (256, 256), # Можно любой другой размер картинки\n",
    "             \"chunk_size\": 512, # количество изображений в блоке, загружаемых в оперативную память\n",
    "            }\n",
    "\n",
    "# Конфигурации обучающей и тестовой выборок отличаются файлов аннотации\n",
    "config_train = {\"annotation_file\":  \"SegTask/trainval.txt\"} | default_config\n",
    "config_test = {\"annotation_file\": \"SegTask/test.txt\"} | default_config\n",
    "\n",
    "train_data_hdf5 = prepare_dataset(config_train, storage_hdf5)\n",
    "train_data_memmap = prepare_dataset(config_train, storage_memmap)\n",
    "train_data_raw = prepare_dataset(config_train, storage_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udjzwt7I_R_h"
   },
   "source": [
    "### 1.2 Создание Dataset и DataLoader (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZO8GtXd_R_h"
   },
   "source": [
    "Pytorch [предоставляет](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) нам удобные обертки Dataset и DataLoader для наших данных, которые эффективно нарезают наш датасет на `batches` (блоки) заданного размера, а также параллелизуют процесс чтения на `num_workers` нитей.\n",
    "\n",
    "Также для дальнейшей работы нам понадобится [аугментация](https://pytorch.org/vision/stable/transforms.html) данных. Ее цель заключается в еще большем расширении обучающей выборки путем применения преобразований над изображениями, которые изменяют их абсолютные значения пикселей, но не нарушают их информационное наполнение.\n",
    "\n",
    "Например, преобразование [ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter) способно изменить яркость изображения на случайное число, что не изменяет его контекст. Однако, преобразование  [RandomCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop) не рекомендуется, посколько есть шанс, что мордочка животного не попадет в фото и класс животного будет неоднозначен. Таким образом, при каждом вызове объекта из обучающей выборки к нему будет применяться случайное преобразование/серия случайных преобразований. **Обратите внимание, что преобразование изображения должно быть согласованным с его сегментационной маской**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvH5CAfJ_R_h"
   },
   "source": [
    "**Требуется реализовать предлагаемые ниже преобразования аугментации:**\n",
    "- `HorizontalFlip` (0.25 балла)\n",
    "- `ColorJitter` (0.25 балла)\n",
    "- `RandomPerspective` (0.5 балла)\n",
    "\n",
    "Для каждого из указанных преобразований требуется написать магический метод `__call__`, который позволяет обращаться к объекту класса (преобразованию), как к функции (функтор из C++):\n",
    "```Python\n",
    "# инициализация\n",
    "obj = Example()\n",
    "# вызывается __call__\n",
    "obj()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Iosqmkq_R_h"
   },
   "outputs": [],
   "source": [
    "# Предлагается использовать эти функции\n",
    "# Самому писать процедуры отражения картинки по вертикали/горизонтали или цветокоррекции не надо!\n",
    "from torchvision.transforms.functional import hflip\n",
    "from torchvision.transforms.functional import perspective\n",
    "from torchvision.transforms import ColorJitter as CJ\n",
    "\n",
    "\n",
    "class HorizontalFlip():\n",
    "    def __init__(self, prob: float):\n",
    "        self.p = prob\n",
    "\n",
    "    def __call__(self, pair: tuple[Image_t, Image_t]) -> tuple[Image_t, Image_t]:\n",
    "        \"\"\"\n",
    "        `pair` содержит пару (изображение, сегментационная маска)\n",
    "        * Почитайте: https://pytorch.org/vision/main/generated/torchvision.transforms.functional.hflip.html\n",
    "        \"\"\"\n",
    "        if (np.random.binomial(1, self.p)):\n",
    "            pair = (hflip(pair[0]), hflip(pair[1]))\n",
    "        return pair\n",
    "\n",
    "\n",
    "class ColorJitter():\n",
    "    def __init__(self, prob: float, param: tuple[float, ...]):\n",
    "        self.p = prob\n",
    "        self.CJ = CJ(*param)\n",
    "\n",
    "    def __call__(self, pair: tuple[Image_t, Image_t]) -> tuple[Image_t, Image_t]:\n",
    "        \"\"\"\n",
    "        `pair` содержит пару (изображение, сегментационная маска)\n",
    "        * Почитайте: https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html\n",
    "        * Сделайте по аналогии с HorizontalFlip\n",
    "        \"\"\"\n",
    "        <Your Code>\n",
    "        return pair\n",
    "\n",
    "\n",
    "class RandomPerspective():\n",
    "    def __init__(self, prob: float, param: float):\n",
    "        self.p = prob\n",
    "        self.distortion_scale = param\n",
    "\n",
    "    def __call__(self, pair: tuple[Image_t, Image_t]) -> tuple[Image_t, Image_t]:\n",
    "        \"\"\"\n",
    "        `pair` содержит пару (изображение, сегментационная маска)\n",
    "        * Почитайте: https://pytorch.org/vision/main/generated/torchvision.transforms.RandomPerspective.html\n",
    "        * Сделайте по аналогии с HorizontalFlip\n",
    "        * Используйте `transforms.RandomPerspective.get_params` и `torchvision.transforms.functional.perspective`\n",
    "          рекомендуем параметры: fill=0, interpolation=IM.NEAREST\n",
    "        \"\"\"\n",
    "        <Your Code>\n",
    "        return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqWWjcMk_R_i"
   },
   "source": [
    "Применим реализованные преобразования и убедимся в их работоспособности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCUgpobM_R_i"
   },
   "outputs": [],
   "source": [
    "img_idx = np.random.randint(0, 100)\n",
    "f, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "pair = train_data_hdf5[img_idx]\n",
    "\n",
    "imgs2draw = {\"Source\": pair,\n",
    "            \"HorizontalFlip\": HorizontalFlip(1.0)(pair),\n",
    "            \"ColorJitter\": ColorJitter(1.0, (0.4, 0.4, 0.4))(pair),\n",
    "            \"RandomPerspective\": RandomPerspective(1.0, 0.25)(pair)\n",
    "}\n",
    "for idx, (name, pair) in enumerate(imgs2draw.items()):\n",
    "    ax[0, idx].imshow(pair[0])\n",
    "    ax[0, idx].set_title(name, fontsize=20)\n",
    "    ax[1, idx].imshow(colorize(np.array(pair[1])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF7c-M_4_R_i"
   },
   "source": [
    "Далее описываем наш класс `SegmentationData` и операции приведения изображений типа PIL.Image к pytorch тензорам с ImageNet `нормализацией`. ImageNet нормализация - это частный случай [Standard normalization](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html), в котором поканальное среднее (цветовые каналы red, green, blue) и поканальное среднеквадратическое отклонение вычислены на [огромной выборке изображений](https://en.wikipedia.org/wiki/ImageNet).\n",
    "\n",
    "**Ответьте на вопрос:** `А для чего нужно применять нормализацию к изображениям?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kyMpJBX_R_j"
   },
   "source": [
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzkVONTn_R_j"
   },
   "outputs": [],
   "source": [
    "# Определяем устройство для вычислений (!желательно GPU!)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# конфигурации для преобразования картинок в тензора и обратно (для визуализации и обучения)\n",
    "t_dict = {\n",
    "    \"forward_input\": transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Lambda(lambda x: x.float().to(DEVICE)/255.0),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"backward_input\": transforms.Compose([\n",
    "        transforms.Normalize(mean=[0.0, 0.0, 0.0],\n",
    "                                     std=[1./0.229, 1./0.224, 1./0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n",
    "                                     std=[1.0, 1.0, 1.0]),\n",
    "        transforms.Lambda(lambda x: x.permute(1, 2, 0).cpu().numpy())\n",
    "    ]),\n",
    "    \"forward_target\": transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Lambda(lambda x: x.long().squeeze().to(DEVICE)),\n",
    "    ]),\n",
    "    \"backward_target\": transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.cpu().numpy())\n",
    "    ]),\n",
    "    \"augment\": transforms.Compose([\n",
    "        HorizontalFlip(0.5),\n",
    "        ColorJitter(0.5, (0.4, 0.4, 0.4)),\n",
    "        RandomPerspective(0.5, 0.25)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset_raw: Type[storage_class], transforms: dict, train_flag: bool = True):\n",
    "        \"\"\"\n",
    "        Наследуем весь функционал из `Dataset` для наших данных `dataset_raw`\n",
    "        `transforms` содержит преобразования PIL.Image <-> torch.tensor и аугментации\n",
    "        `train_flag` регулирует аугментацию данных (для тестовой выборки она не нужна)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_raw = dataset_raw\n",
    "        self.transforms = transforms\n",
    "        self.train_flag = train_flag\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_raw.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image_t, Image_t]:\n",
    "        input, target = self.dataset_raw[idx]\n",
    "\n",
    "        if (self.train_flag):\n",
    "            input, target = self.transforms[\"augment\"]((input, target))\n",
    "\n",
    "        return self.transforms[\"forward_input\"](input), self.transforms[\"forward_target\"](target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRyNAaKz_R_j"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Разделяем выборку на обучающую и валидационную\n",
    "def split_train_val(train_data: Type[storage_class], train_portion: float = 0.8):\n",
    "    \"\"\"\n",
    "    `train_data` предобработанные данные\n",
    "    `train_portion` доля объектов, которая будет приходиться на обучающую выборку\n",
    "    \"\"\"\n",
    "    trainval_dataset = SegmentationDataset(train_data, t_dict, train_flag=True)\n",
    "\n",
    "    train_size = int(len(trainval_dataset) * train_portion)\n",
    "    val_size = len(trainval_dataset) - train_size\n",
    "    return random_split(trainval_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset_hdf5, val_dataset_hdf5 = split_train_val(train_data_hdf5)\n",
    "train_dataset_memmap, val_dataset_memmap = split_train_val(train_data_memmap)\n",
    "train_dataset_raw, val_dataset_raw = split_train_val(train_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb7FGjRU_R_j"
   },
   "source": [
    "Отрисуем случайное изображение (после применения случайных преобразований аугментации):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7ml1W5Z_R_j"
   },
   "outputs": [],
   "source": [
    "img_idx = np.random.randint(0, 100)\n",
    "draw(train_dataset_hdf5[img_idx], t_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzqnrmp5_R_k"
   },
   "outputs": [],
   "source": [
    "dataloader_config = {\n",
    "    \"batch_size\": 16,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "train_dataloader_hdf5 = DataLoader(train_dataset_hdf5, **dataloader_config)\n",
    "val_dataloader_hdf5 = DataLoader(val_dataset_hdf5, **dataloader_config)\n",
    "\n",
    "train_dataloader_memmap = DataLoader(train_dataset_memmap, **dataloader_config)\n",
    "val_dataloader_memmap = DataLoader(val_dataset_memmap, **dataloader_config)\n",
    "\n",
    "train_dataloader_raw = DataLoader(train_dataset_raw, **dataloader_config)\n",
    "val_dataloader_raw = DataLoader(val_dataset_raw, **dataloader_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLWueo8k_R_k"
   },
   "source": [
    "### 1.3 Замер скорости чтения датасета с диска (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0sbLVZv_R_k"
   },
   "source": [
    "**Замерьте время чтения нашего датасета для каждого из форматов хранения:** (Все SpeedTests вместе занимают около 6 минут!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_yCipVt_R_k"
   },
   "outputs": [],
   "source": [
    "def speedtest(dataloader: Type[DataLoader]) -> None:\n",
    "    for batch in dataloader:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOShZC9U_R_k"
   },
   "outputs": [],
   "source": [
    "%timeit speedtest(train_dataloader_hdf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tf88Uyhr_R_l"
   },
   "outputs": [],
   "source": [
    "%timeit speedtest(train_dataloader_memmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIrhnS4n_R_l"
   },
   "outputs": [],
   "source": [
    "%timeit speedtest(train_dataloader_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3WdxhIX_R_l"
   },
   "source": [
    "**Ответьте на вопрос: `Какой формат оказался самым эффективным по скорости? Почему?`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFZZ5UF2_R_l"
   },
   "source": [
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx8jBHv__R_m"
   },
   "source": [
    "**Создайте тестовый Dataloader победившего по скорости формата.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx9gQrhY_R_m"
   },
   "outputs": [],
   "source": [
    "dataloader_config = {\n",
    "    \"batch_size\": 16,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "# используйте: `config_test`, `prepare_dataset`, `SegmentationDataset`, новый `dataloader_config`\n",
    "test_dataloader = <Your Code>\n",
    "\n",
    "# добавьте новые переменные с выбранным форматом\n",
    "train_dataloader = train_dataloader_<Chosen Format>\n",
    "val_dataloader = val_dataloader_<Chosen Format>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmOHuo0c_R_m"
   },
   "source": [
    "## Часть 2: Реализация функций потерь, метрик и декодировщика PSPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQBPxZz_R_n"
   },
   "source": [
    "Ранее вас познакомили с архитектурой Unet - сверточными автокодировщиком, применяемом в области сегментации изображений. В данном задании мы разберем более продвинутую архитектуру сети сегментации [PSPNet](https://arxiv.org/abs/1612.01105). Отличительной особенностью этой сети является `Pyramid Pooling Module`, который, в отличие от Unet, позволяет учитывать `глобальный` контекст изображения при формировании признаков его `локальных` областей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiODgv27_R_n"
   },
   "source": [
    "Рассмотрим предлагаемую архитектуру `PSPNet-подобной сети`:\n",
    "![picture](https://drive.google.com/uc?id=1WNumWndaJAbZBch0dLf6iT8KiSdUbIFG)\n",
    "\n",
    "В качестве кодировщика `Encoder` будем брать предобученную [ResNeXt](https://pytorch.org/hub/pytorch_vision_resnext/) сеть. Будем его использовать для получения двух глубинных представлений нашего входого изображения `x`:\n",
    "- **выход `x_main`** (на рисунке: верхний выход Encoder-а ) - \"среднее\" промежуточное представление, компромисс между низкоуровневыми признаками (цвет, контуры объектов, штрихи) и высокоуровневыми признаками (абстрактные признаки, отражающие семантику изображения)\n",
    "- **выход `x_supp`** (на рисунке: нижний выход Encoder-а ) - финальное представление, содержащее самые высокоуровневые признаки, в которых значительно утеряна информация о точном простанственном расположении объектов\n",
    "\n",
    "Подобное разбиение выхода на 2 потока объясняется необходимостью в закодированной информации о пространственном расположении объектов (`x_main`) и вспомогательной информации о семантике всего изображения в целом (`x_supp`) для задачи семантической сегментации. Мы не можем себе позволить использовать лишь выход `x_supp`, как это делается, например, в задачах классификации, ведь от нас требуется дополнительное знание о расположении этого объекта на изображении.\n",
    "\n",
    "**Ваша задача состоит в написании декодировщика `Decoder`, а именно в написании блоков:**\n",
    "- **`Pyramid Pooling Module`**. Эта процедура необходима для извлечения глобального контекста разных масштабов, которого не хватает классическим сверточным нейронным сетям (локальный контекст в пределах размера фильтра).\n",
    "    * К входному тензору `x_main` параллельно применяется несколько операций пулинга разных масштабов и получаются представления таких размеров: `1x1`, `2x2`, `3x3` и `6x6`.\n",
    "    * Каналы этих промежуточных тензоров эффективно сжимаем (при помощи `nn.Conv2d c размером фильтра 1x1`). Делаем это для сжатия информации, а также для индивидуального взвешивания каждого тензора (назовем их тензорами глобального контекста разного масштаба).\n",
    "    * Затем восстанавливаем полученные тензоры до исходных размеров с помощью интерполяции.\n",
    "    * Выходной тензор получаем конкатенацией этих глобальных контекстов. Каждый контекст содержит информацию о всем изначальном изображении с разными уровнями детализации. Требуется реализовать `forward` этап этого блока. Для уточнения информации можно обратиться к [статье](https://arxiv.org/abs/1612.01105).\n",
    "- **`Supplementary Module`** осуществляет нелинейное преобразование над входным тензором `x_supp` с понижением числа каналов до числа на выходе модуля **`Pyramid Pooling Module`**. Вариант архитектуры этого преобразования (композиции слоев) уже предложен, но, при желании, вы можете с ним экспериментировать\n",
    "- **`Upsample Module`** осуществляет нелинейные преобразования над входным тензором с понижением числа каналов, которые чередуются с увеличением с помощью интерполяции пространственных размерностей в 2 раза. В результатае преобразований выход этого блока имеет ту же пространственную размерность, что и входное в кодировщик изображение.\n",
    "- **`Segmentation Head`** нелинейно преобразует входной тензор в тензор score'ов. Выходной тензор для каждого пикселя имеет `num_classes` score'ов (в нашем случае 3). Индекс максимального score'а для заданного пикселя есть его метка класса (0, 1 или 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDWiTlh-_R_n"
   },
   "source": [
    "### 2.1 Кодировщик и декодировщик PSPNet-подобной сети (3 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVvrQ7QF_R_n"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet\n",
    "\n",
    "pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)\n",
    "\n",
    "# Выставляем evaluation mode (влияет на поведение таких слоев как BatchNorm2d, Dropout)\n",
    "pretrained_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY6M-EFl_R_o"
   },
   "source": [
    "Так как кодировщик используется предобученный, то требуется зафиксировать (заморозить) веса, чтобы по ним не тек градиент. Этим мы гарантируем, что кодировщик не изменяется в ходе обучения автокодировщика, а также экономим вычислительные ресурсы (граф градиента кодировщика не строится)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cj72Yddx_R_o"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, pretrained_model: Type[ResNet]):\n",
    "        \"\"\"\n",
    "        Извлекает предобученные именованные слои кодировщика `pretrained_model`\n",
    "        Разделяет слои на `main` и `supp` потоки (см. архитектуру выше)\n",
    "\n",
    "        Вход: тензор (Batch_size, 3, Height, Width)\n",
    "\n",
    "        Выход: x_main тензор (Batch_size, 512, Height // 8, Width // 8)\n",
    "        Выход: x_supp тензор (Batch_size, 2048, Height // 32, Width // 32)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_main = nn.Sequential()\n",
    "        for name, child in list(pretrained_model.named_children())[:-4]:\n",
    "            print(f\"Pretrained main module {name} is loaded\")\n",
    "            self.encoder_main.add_module(name, child)\n",
    "\n",
    "        self.encoder_supp = nn.Sequential()\n",
    "        for name, child in list(pretrained_model.named_children())[-4:-2]:\n",
    "            print(f\"Pretrained supp module {name} is loaded\")\n",
    "            self.encoder_supp.add_module(name, child)\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Замораживает веса кодировщика\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Размораживает веса кодировщика\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "        x_main = self.encoder_main(x)\n",
    "        x_supp = self.encoder_supp(x_main)\n",
    "        return x_main, x_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gO0Dfrju_R_o"
   },
   "outputs": [],
   "source": [
    "encoder = EncoderBlock(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2FJRsd1_R_o"
   },
   "source": [
    "**Для оценки сложности модели нам понадобится функция подсчета числа ее параметров, для этого используйте метод [model.parameters()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters). Реализуйте ее ниже:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d1IvuIw_R_o"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model: Type[nn.Module]) -> int:\n",
    "    \"\"\"\n",
    "    Считает число весов в модели `model`, для которых требуется градиент\n",
    "    * Используйте model.parameters() чтобы получить список параметров:\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters\n",
    "    * Используйте requires_grad, для проверки, считается ли для данного параметра градиент:\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    <Your Code>\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmKpcPoq_R_q"
   },
   "source": [
    "Убедимся, что метод `.freeze()` успешно замораживает веса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU63BAy4_R_q"
   },
   "outputs": [],
   "source": [
    "print(\"Encoder #parameters before freeze():\", count_parameters(encoder))\n",
    "encoder.freeze()\n",
    "print(\"Encoder #parameters after freeze():\", count_parameters(encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20VS7Eev_R_q"
   },
   "source": [
    "* **Реализуйте `PyramidPoolingModule`, `Upsample` и `SegmentationHead` (по 1 баллу)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGzypapg_R_q"
   },
   "outputs": [],
   "source": [
    "class PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bin_sizes: tuple[int, ...]):\n",
    "        \"\"\"\n",
    "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "        `bin_sizes` - пространственные размерности для каждой пулинг операции\n",
    "        Пример: bin_sizes = (1, 2, 3, 6).\n",
    "\n",
    "        Выход: тензор (Batch_size, `in_channels` + len(`bin_sizes`) * `out_channels`, Height, Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bins = []\n",
    "\n",
    "        for bin_size in bin_sizes:\n",
    "            self.bins.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(bin_size), # почитайте: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1), # почитайте: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "                nn.BatchNorm2d(out_channels), # почитайте: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        self.bins = nn.ModuleList(self.bins)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        h, w = x.shape[2:]\n",
    "        out = [x,]\n",
    "        \"\"\"\n",
    "        * Пройдитесь циклом по self.bins и примените исходное изображение отдельно\n",
    "          к каждому  блоку операций Pooling + Conv + BatchNorm + ReLU\n",
    "        * После применения не забудьте на каждый выход сделать `Upscale`\n",
    "          (используйте torch.functional.interpolate)\n",
    "          и добавить результат в список `out`\n",
    "        * Сконкатенируйте тензоры из out в один большой тензор по размерности с каналами\n",
    "          с помощью torch.cat\n",
    "        \"\"\"\n",
    "        <Your Code>\n",
    "\n",
    "\n",
    "class SupplementaryModule(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "\n",
    "        Выход: тензор (Batch_size, `out_channels`, Height, Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.suppl = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.Conv2d(in_channels // 2, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.suppl(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "\n",
    "        Выход: тензор (Batch_size, `out_channels`, 2 * Height, 2 * Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Рекомендуем попробовать Conv2d (kernel: 3x3, padding=1) + BatchNorm2d + ReLU\n",
    "        self.us_transform = nn.Sequential(\n",
    "            <Your Code>\n",
    "        )\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Увеличьте входное изображение в два раза, а затем примените us_transform\n",
    "        Используйте torch.functional.interpolate c mode='bilinear', align_corners=True\n",
    "        \"\"\"\n",
    "        <Your Code>\n",
    "\n",
    "\n",
    "class UpsampleModule(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "\n",
    "        Выход: тензор (Batch_size, `out_channels`, 8 * Height, 8 * Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            Upsample(in_channels, in_channels // 4),\n",
    "            Upsample(in_channels // 4, in_channels // 8),\n",
    "            Upsample(in_channels // 8, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lqgbHy__R_r"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bin_sizes: tuple[int, ...], dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Вход  x_main: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "        Вход  x_supp: тензор (Batch_size, 4 * `in_channels`, Height // 4, Width // 4)\n",
    "\n",
    "        Выход: тензор (Batch_size, `out_channels`, 8 * Height, 8 * Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert in_channels % len(bin_sizes) == 0\n",
    "\n",
    "        self.PPM = PyramidPoolingModule(in_channels, in_channels // len(bin_sizes), bin_sizes)\n",
    "        self.SM = SupplementaryModule(4 * in_channels, 2 * in_channels, dropout)\n",
    "        self.UM = UpsampleModule(4 * in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_main: torch.tensor, x_supp: torch.tensor) -> torch.tensor:\n",
    "        h_supp, w_supp = x_supp.shape[2:]\n",
    "        x_supp = F.interpolate(input=x_supp, size=(4 * h_supp, 4 * w_supp), mode='bilinear', align_corners=True)\n",
    "\n",
    "        x_supp = self.SM(x_supp)\n",
    "        x_main = self.PPM(x_main)\n",
    "\n",
    "        out = self.UM(torch.cat([x_main, x_supp], dim=1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Вычисляет score для каждого из классов\n",
    "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
    "\n",
    "        Выход: тензор (Batch_size, `num_classes`, Height, Width)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Рекомендуем попробовать: BatchNorm2d + ReLU + Dropout2d + Conv2d (1 x 1)\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            <Your Code>\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor, x_supp: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        На будущее зададим фиктивный аргумент `x_supp`, который пока не будем использовать\n",
    "        \"\"\"\n",
    "        return self.segmentation_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d0LCMID_R_s"
   },
   "source": [
    "### 2.2 Реализация метрик (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJc-D1FC_R_s"
   },
   "source": [
    "В задаче сегментации для оценки предсказательной способности нейронной сети, в основном, используют следующие метрики:\n",
    "\n",
    "Пусть $\\mathrm{P}$ обозначает прогноз сег. маски (Prediction), $\\mathrm{S}$ обозначает score'ы для каждого класса сег. маски (Scores), а $\\mathrm{T}$ означает сег. маску (Target). Тогда:\n",
    "- **Intersection over Union metric (коэффициент Жаккара)**:\n",
    "$$\n",
    "\\mathrm{IoU}(P, T) = \\dfrac{\\sum_{i=1}^{M}\\sum_{j=1}^{N}[P_{ij}*T_{ij}]}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} [P_{ij} + T_{ij} - P_{ij}*T_{ij}]}\\text{, где } P, T \\in \\{0, 1\\}^{M \\times N}\n",
    "$$\n",
    "- **Recall metric (полнота)**:\n",
    "$$\n",
    "\\mathrm{Recall}(P, T) = \\dfrac{\\sum_{i=1}^{M}\\sum_{j=1}^{N}[P_{ij} * T_{ij}]}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} T_{ij}}\\text{, где } P, T \\in \\{0, 1\\}^{M \\times N}\n",
    "$$\n",
    "Указанные выше метрики расписаны для случая бинарной сегментации, которая нам не подходит. Обобщим их на случай мультиклассовой сегментации: представим K-классовую задачу как K двухклассовых, а затем `макро-` или `микро-`усредним для них метрики. **Требуется реализовать мультиклассовые варианты указанных метрик с поддержкой макро- и микро-усреднения (по 1 баллу). Обратите внимание, что метрики рассчитываются для каждого элемента из батча. За редуцирование метрик вдоль размерности батча отвечает аргумент `reduce` (см. ниже).**\n",
    "\n",
    "Также для обучения будем использовать две разные, но схожие функции потерь:\n",
    "- Cross Entropy Loss (кросс-энтропия):\n",
    "$$\n",
    "\\mathrm{CE}(S, T) = - \\dfrac{1}{MN}\\sum_{c=1}^{K}\\sum_{i=1}^{M}\\sum_{j=1}^{N} \\big[\\log \\mathrm{Softmax}(S)_{cij}*\\mathbb{I}[T_{ij} == c]\\big]\\text{, где } S \\in \\mathbb{R}^{K \\times M \\times N}, T \\in \\{1, ..., K\\}^{M \\times N}\n",
    "$$\n",
    "- [Focal Loss](https://arxiv.org/abs/1708.02002):\n",
    "$$\n",
    "\\mathrm{FL}(S, T) = - \\dfrac{1}{MN}\\sum_{c=1}^{K}\\sum_{i=1}^{M}\\sum_{j=1}^{N} \\big[(1 - \\mathrm{Softmax}(S)_{cij})^{\\gamma}*\\log \\mathrm{Softmax}(S)_{cij}*\\mathbb{I}[T_{ij} == c]\\big]\\text{, где } S \\in \\mathbb{R}^{K \\times M \\times N}, T \\in \\{1, ..., K\\}^{M \\times N}, \\gamma \\in \\mathbb{R}_{+} - \\text{гиперпараметр}\n",
    "$$\n",
    "\n",
    "**Функции потерь реализовывать не требуется**. Также всюду необходимо обеспечить корректную обработку значений `ignore_index`, которые в нашем случае равны 255 (не участвуют в расчете метрик/функций потерь). Если представители некоторых классов в $\\mathrm{T}$ отсутствуют, то учитывать эти классы при макро-усреднении не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEO27WzN_R_s"
   },
   "outputs": [],
   "source": [
    "class MetricsCollection():\n",
    "    def __init__(self, num_classes: int, ignore_index: int = 255):\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def IoUMetric(self, prediction: torch.tensor, target: torch.tensor, average: str = \"macro\", reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
    "        \"\"\"\n",
    "        `prediction` предсказанная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `average` тип мультклассового усреднения\n",
    "        `reduce` редукция значений метрики вдоль размерности Batch; None - без редукции\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        micro - суммируем знаменатель для всех классов, числитель для всех классов\n",
    "                и делим одно на другое\n",
    "        macro - считаем метрику по каждому классу отдельно, затем усредняем\n",
    "        \"\"\"\n",
    "        assert average in [\"micro\", \"macro\"]\n",
    "\n",
    "        \"\"\"\n",
    "        sum - сумма метрик по всем картинкам\n",
    "        mean - среднее метрик по всем картинкам\n",
    "        none - массив метрик по всем картинкам\n",
    "        \"\"\"\n",
    "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
    "        <Your Code>\n",
    "\n",
    "    def RecallMetric(self, prediction: torch.tensor, target: torch.tensor, average: str = \"macro\", reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
    "        \"\"\"\n",
    "        `prediction` предсказанная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `average` тип мультклассового усреднения\n",
    "        `reduce` редукция значений метрики вдоль размерности Batch; None - без редукции\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        micro - суммируем знаменатель для всех классов, числитель для всех классов\n",
    "                и делим одно на другое\n",
    "        macro - считаем метрику по каждому классу отдельно, затем усредняем\n",
    "        \"\"\"\n",
    "        assert average in [\"micro\", \"macro\"]\n",
    "\n",
    "        \"\"\"\n",
    "        sum - сумма метрик по всем картинкам\n",
    "        mean - среднее метрик по всем картинкам\n",
    "        none - массив метрик по всем картинкам\n",
    "        \"\"\"\n",
    "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
    "        <Your Code>\n",
    "\n",
    "    def FocalLoss(self, scores: torch.tensor, target: torch.tensor, reduce: str = \"mean\", gamma: float = 1.) -> Union[torch.tensor, float]:\n",
    "        \"\"\"\n",
    "        `scores` score'ы каждого класса сегментационной маски размера (Batch_size, num_classes, Height, Width)\n",
    "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `reduce` редукция значений функции потерь вдоль размерности Batch; None - без редукции\n",
    "        \"\"\"\n",
    "        assert scores.shape[1] == self.num_classes\n",
    "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
    "\n",
    "        ce_loss = F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"none\")\n",
    "        coef = (1 - torch.exp(-ce_loss))**gamma\n",
    "        focal_loss = coef * ce_loss\n",
    "        norm = (focal_loss.numel() - (target == self.ignore_index).sum())\n",
    "\n",
    "        if (reduce == \"sum\"):\n",
    "            return focal_loss.sum() / norm * scores.shape[0]\n",
    "        elif (reduce == \"mean\"):\n",
    "            return focal_loss.sum() / norm\n",
    "        else:\n",
    "            return focal_loss.sum(dim=[1, 2]) / norm * scores.shape[0]\n",
    "\n",
    "    def CrossEntropyLoss(self, scores: torch.tensor, target: torch.tensor, reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
    "        \"\"\"\n",
    "        `scores` score'ы каждого класса сегментационной маски размера (Batch_size, num_classes, Height, Width)\n",
    "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
    "        `reduce` редукция значений функции потерь вдоль размерности Batch; None - без редукции\n",
    "        \"\"\"\n",
    "        assert scores.shape[1] == self.num_classes\n",
    "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
    "\n",
    "        if (reduce == \"sum\"):\n",
    "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"mean\") * scores.shape[0]\n",
    "        elif (reduce == \"mean\"):\n",
    "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"mean\")\n",
    "        else:\n",
    "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"none\")\n",
    "\n",
    "    @classmethod\n",
    "    def ListMetrics(cls):\n",
    "        return [method for method in dir(cls) if (method.endswith(\"Metric\"))]\n",
    "\n",
    "    @classmethod\n",
    "    def ListLosses(cls):\n",
    "        return [method for method in dir(cls) if (method.endswith(\"Loss\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGyYUxM2_R_t"
   },
   "outputs": [],
   "source": [
    "metric_class = MetricsCollection(num_classes=3, ignore_index=255)\n",
    "\n",
    "prediction = torch.tensor([[[0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n",
    "                           [[0, 0, 0, 0], [0, 2, 2, 0], [0, 2, 0, 0], [0, 0, 0, 0]]])\n",
    "\n",
    "target = torch.tensor([[[0, 0, 0, 0], [0, 1, 255, 0], [0, 1, 255, 0], [0, 0, 0, 0]],\n",
    "                       [[0, 0, 0, 0], [0, 255, 2, 0], [0, 255, 2, 0], [0, 0, 0, 0]]])\n",
    "\n",
    "assert np.isclose(metric_class.RecallMetric(prediction, target, \"micro\", \"mean\").item(), 0.9286, atol=1e-3)\n",
    "assert np.isclose(metric_class.RecallMetric(prediction, target, \"macro\", \"mean\").item(), 0.7500, atol=1e-3)\n",
    "assert np.isclose(metric_class.IoUMetric(prediction, target, \"micro\", \"mean\").item(), 0.8667, atol=1e-3)\n",
    "assert np.isclose(metric_class.IoUMetric(prediction, target, \"macro\", \"mean\").item(), 0.7115, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85cXRHCw_R_t"
   },
   "source": [
    "**Ответьте на вопрос (№1):** `Что говорит о предсказательной способности нашей сети ситуация: высокий Recall и низкий IoU для некоторого класса? Возможна ли обратная ситуация?`\n",
    "\n",
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImvBP-G0_R_t"
   },
   "source": [
    "**Ответьте на вопрос (№2):** `Какой вид усреднения правильней использовать в нашей задаче: макро и микро? Почему?`\n",
    "\n",
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy_g-BoO_R_u"
   },
   "source": [
    "**Ответьте на вопрос (№3):** `В чем преимущество Focal Loss перед Cross Entropy Loss? Что контроллирует гиперпараметр 𝛾 в Focal Loss?`\n",
    "\n",
    "**Ваш ответ:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHaKI56l_R_u"
   },
   "source": [
    "## Часть 3: Обучение PSPNet, эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfzVJMy6_R_v"
   },
   "source": [
    "Теперь **осталось лишь собрать все написанное ранее воедино и обучить нашу сеть**. Чтобы контроллировать процесс обучения нашей сети, будем вычислять усредненные метрики и функции потерь на валидационной выборке. Для удобства отображения информации **воспользуемся инструментом `tensorboard`**. Для этого заведем объект класса `SummaryWriter`, который создаст и откроет на запись специальный `event` файл для [tensorboard](https://pytorch.org/docs/stable/tensorboard.html). Для визуализации содержимого вводится команда `tensorboard --logdir=<PATH>` в терминале. Если возникла необходимость в мониториге нескольких tensorboard, то каждому из них требуется присвоить свой уникальный порт `--port <PORT>`. [Пример](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb#scrollTo=lFKETpE2F2oE) использования tensorboard на Google Colab.\n",
    "\n",
    "**Требуется написать методы `train_model` и `test_model`. Вся конфигурация обучения хранится в словаре `train_config`. При желании его можно дополнить чем-то своим.**\n",
    "\n",
    "**К вашему решению потребуется прикрепить логи tensorboard. Чтобы облегчить процедуру проверки настоятельно рекомендуется пользоваться `inline-tensorboard`:**\n",
    "```\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFbUf6cR_R_v"
   },
   "source": [
    "### 3.1 Реализация процедур обучения/тестирования сети (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEieZVVd_R_v"
   },
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, pretrained_model: Type[ResNet], HeadBlock: Type[nn.Module], num_classes: int, train_config: dict, bin_sizes: tuple[int, ...] = (1, 2, 3, 6)):\n",
    "        \"\"\"\n",
    "        `pretrained_model` модель предобученного кодировщика\n",
    "        `Head` класс блока, оценивающего score'ы для каждого класса сегментационной маски\n",
    "        `num_class` число классов сегментации\n",
    "        `train_config` словарь с конфигурацией процесса обучения сети\n",
    "        `bin_sizes` пространственные размеры к которым сводит пулинг в блоке PPM\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderBlock(pretrained_model)\n",
    "        self.encoder.freeze()\n",
    "        self.decoder = DecoderBlock(512, 128, bin_sizes)\n",
    "        self.head = HeadBlock(128, num_classes)\n",
    "\n",
    "        self.train_config = train_config\n",
    "        self.metric_class = train_config[\"metric_class\"]\n",
    "        self.optimizer = train_config[\"optimizer\"](self.parameters(), **train_config[\"optimizer_params\"])\n",
    "        self.scheduler = train_config[\"scheduler\"](self.optimizer, **train_config[\"scheduler_params\"])\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
    "        # Для гарантии отсутствия градиентов по кодировщику\n",
    "        with torch.no_grad():\n",
    "            x_main, x_supp = self.encoder(x)\n",
    "        out = self.decoder(x_main, x_supp)\n",
    "        out = self.head(out, x_supp)\n",
    "        return out, torch.argmax(out.detach(), dim=1)\n",
    "\n",
    "    def write_val_metrics(self, val_metrics: dict, iter_num: int, norm: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Записывает усредненные значения метрик/функций потерь в tensorboard\n",
    "\n",
    "        `val_metrics` словарь с ключами \"название_метрики/функции потерь\" и их значениями\n",
    "        `iter_num` номер глобальной итерации (по формуле #всего_итераций * номер_эпохи + номер_итерации)\n",
    "        `norm` фактор нормализации; для усреднения равен числу объектов в валидационной выборке\n",
    "        \"\"\"\n",
    "        for method, value in val_metrics.items():\n",
    "            self.train_config[\"writer\"].add_scalar(f\"Mean {method}\", np.round(val_metrics[method].item()/norm, 2), iter_num)\n",
    "\n",
    "    def validate_model(self, val_dataloader: Type[DataLoader], iter_num: int) -> None:\n",
    "        \"\"\"\n",
    "        Валидирует текущую модель и вычисляет соответствующие метрики/функции потерь\n",
    "\n",
    "        `val_dataloader` валидационная выборка\n",
    "        `iter_num` номер глобальной итерации (по формуле #всего_итераций * номер_эпохи + номер_итерации)\n",
    "        \"\"\"\n",
    "        # Выставляет декодировщик в режим валидации (влияет на поведение BatchNorm2d и Dropout)\n",
    "        self.decoder.eval()\n",
    "        self.head.eval()\n",
    "\n",
    "        # Инициализация словаря метрик/функций потерь\n",
    "        val_metrics = dict([(method, 0.0) for method in (self.metric_class.ListMetrics() + self.metric_class.ListLosses())])\n",
    "\n",
    "        # Обязательно считать с контекстным менеджером torch.no_grad()\n",
    "        # Даже если мы не делаем шаг оптимизации, мы экономим память (не считаем градиенты)\n",
    "        with torch.no_grad():\n",
    "            for input, target in val_dataloader:\n",
    "                scores, prediction = self.forward(input)\n",
    "                for metric in self.metric_class.ListMetrics():\n",
    "                    val_metrics[metric] += getattr(self.metric_class, metric)(prediction, target, reduce=\"sum\")\n",
    "\n",
    "                for loss in self.metric_class.ListLosses():\n",
    "                    val_metrics[loss] += getattr(self.metric_class, loss)(scores, target, reduce=\"sum\")\n",
    "\n",
    "        # Tensorboard также позволяет сохранять визуализацию наших предсказаний в ходе обучения\n",
    "        figure = draw((input[0], target[0]), t_dict, prediction[0], log=True)\n",
    "        self.train_config[\"writer\"].add_figure(\"image/GT/prediction\", figure, iter_num)\n",
    "\n",
    "        self.write_val_metrics(val_metrics, iter_num, norm=len(val_dataloader.dataset))\n",
    "        # Возвращает режим обучения декодировщика\n",
    "        self.decoder.train()\n",
    "\n",
    "    def train_model(self, train_dataloader: Type[DataLoader], val_dataloader: Type[DataLoader]) -> None:\n",
    "        \"\"\"\n",
    "        Обучает модель на обучающей выборке, периодически (периодичность выставляется в train_config) валидирует на валидационной выборке\n",
    "        В конце каждой эпохи сохраняет модель на диск\n",
    "\n",
    "        `train_dataloader` обучающая выборка\n",
    "        `val_dataloader` валидационная выборка\n",
    "        \"\"\"\n",
    "        # Выставляет режим обучения декодировщика\n",
    "        self.decoder.train()\n",
    "        self.head.train()\n",
    "\n",
    "        for epoch in range(self.train_config[\"num_epochs\"]):\n",
    "            for iter_num, (input, target) in enumerate(train_dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                <Your Code>\n",
    "\n",
    "                if (iter_num % self.train_config[\"validate_each_iter\"] == 0):\n",
    "                    print(f\"Epoch: {epoch+1}/{self.train_config['num_epochs']} || Iter: {iter_num}/{len(train_dataloader)} || Loss: {loss.item()}\")\n",
    "                    self.validate_model(val_dataloader, epoch * len(train_dataloader) + iter_num)\n",
    "\n",
    "            torch.save(self.state_dict(), self.train_config[\"save_model_path\"] + f\"_{epoch+1}.pth\")\n",
    "\n",
    "    def test_model(self, test_dataloader: Type[DataLoader]) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Inference модели на тестовой выборке. Возвращает тензор предсказаний сег.масок и тензор истинных сег.масок\n",
    "\n",
    "        `test_dataloader` тестовая выборка\n",
    "        \"\"\"\n",
    "        # Выставляет декодировщик в режим валидации (влияет на поведение BatchNorm2d и Dropout)\n",
    "        self.decoder.eval()\n",
    "        <Your Code>\n",
    "\n",
    "        return dl_prediction, dl_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnvaJv5g_R_w"
   },
   "source": [
    "### 3.2 Обучение PSPNet, эксперименты (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExrZI8Kz_R_w"
   },
   "source": [
    "Вам приведены начальные значения гиперпараметров сети. Подберите гиперпараметры (если необходимо) и обучите сеть на обе функции потерь `CrossEntropyLoss` и `FocalLoss`. Добейтесь следующих результатов на тестовой выборке хотя бы для одной из них:\n",
    "- **`Mean IoU metric` > 0.82**\n",
    "- **`Mean Recall metric` > 0.92**\n",
    "\n",
    "К вашему решению требуется прикрепить логи tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEEde44u5mOE"
   },
   "source": [
    "**CrossEntropyLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KTBCBqY_R_w"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "train_config = {\n",
    "    \"num_epochs\": 2, # примерное время обучения ~ 20 минут на GPU\n",
    "    \"optimizer\": torch.optim.Adam,\n",
    "    \"optimizer_params\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-5\n",
    "    },\n",
    "    \"loss_fn\": metric_class.CrossEntropyLoss,\n",
    "    \"scheduler\": StepLR,\n",
    "    \"scheduler_params\": {\n",
    "        \"step_size\": 50,\n",
    "        \"gamma\": 0.85\n",
    "    },\n",
    "    \"validate_each_iter\": 10,\n",
    "    \"writer\": SummaryWriter(comment=\"CEloss\"),\n",
    "    \"save_model_path\": \"model_celoss\",\n",
    "    \"metric_class\": metric_class\n",
    "}\n",
    "\n",
    "net = PSPNet(pretrained_model, SegmentationHead, num_classes=3, train_config=train_config).to(DEVICE)\n",
    "print(\"#параметров в сети:\", count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shDBt0Df_R_w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.train_model(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CjKyhHa5Q2D"
   },
   "outputs": [],
   "source": [
    "# Протестируйте модель:\n",
    "net.load_state_dict(torch.load(\"model_celoss_1.pth\")) # Не забудьте поменять версию `_1`-> `_n` если запускаете несколько раз!\n",
    "net.eval();\n",
    "dl_prediction, dl_target = net.test_model(test_dataloader)\n",
    "print(\"Mean IoU metric: \", metric_class.IoUMetric(dl_prediction, dl_target))\n",
    "print(\"Mean Recall metric: \", metric_class.RecallMetric(dl_prediction, dl_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSAn7r55-0Zw"
   },
   "outputs": [],
   "source": [
    "# Примеры работы вами сети:\n",
    "img_idx = np.random.randint(0, 100)\n",
    "for idx, (input, target) in enumerate(test_dataloader):\n",
    "    if (idx < img_idx):\n",
    "        continue\n",
    "    draw((input.squeeze(), target.squeeze()), t_dict, dl_prediction[idx])\n",
    "    plt.pause(0.1)\n",
    "    if (idx == img_idx+2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Tk5H5Ut5htl"
   },
   "source": [
    "**FocalLoss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJiIkAVyAjqp"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "train_config = {\n",
    "    \"num_epochs\": 2, # примерное время обучения ~ 20 минут на GPU\n",
    "    \"optimizer\": torch.optim.Adam,\n",
    "    \"optimizer_params\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-5\n",
    "    },\n",
    "    \"loss_fn\": metric_class.FocalLoss,\n",
    "    \"scheduler\": StepLR,\n",
    "    \"scheduler_params\": {\n",
    "        \"step_size\": 50,\n",
    "        \"gamma\": 0.85\n",
    "    },\n",
    "    \"validate_each_iter\": 10,\n",
    "    \"writer\": SummaryWriter(comment=\"Floss\"),\n",
    "    \"save_model_path\": \"model_floss\",\n",
    "    \"metric_class\": metric_class\n",
    "}\n",
    "\n",
    "net = PSPNet(pretrained_model, SegmentationHead, num_classes=3, train_config=train_config).to(DEVICE)\n",
    "print(\"#параметров в сети:\", count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJkiQEF-AtCi"
   },
   "outputs": [],
   "source": [
    "net.train_model(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcro1bVF_R_x"
   },
   "outputs": [],
   "source": [
    "# Протестируйте вторую модель и сравните метрики:\n",
    "net.load_state_dict(torch.load(\"model_floss_1.pth\"))  # Не забудьте поменять версию `_1`-> `_n` если запускаете несколько раз!\n",
    "net.eval();\n",
    "dl_prediction, dl_target = net.test_model(test_dataloader)\n",
    "print(\"Mean IoU metric: \", metric_class.IoUMetric(dl_prediction, dl_target))\n",
    "print(\"Mean Recall metric: \", metric_class.RecallMetric(dl_prediction, dl_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYUc6CXY_R_y"
   },
   "outputs": [],
   "source": [
    "# Примеры работы вами сети:\n",
    "img_idx = np.random.randint(0, 100)\n",
    "for idx, (input, target) in enumerate(test_dataloader):\n",
    "    if (idx < img_idx):\n",
    "        continue\n",
    "    draw((input.squeeze(), target.squeeze()), t_dict, dl_prediction[idx])\n",
    "    plt.pause(0.1)\n",
    "    if (idx == img_idx+2):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
